---
title: "Network Simulations"
author: "Steve McDonald"
date: "2023-10-08"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Network Simulations

Data simulations are a crucial tool for constructing, sharpening, and testing theories. This is especially true for network data. Below I will present some techniques for simulating different types of networks and examining distributions from network simulations. 

First, let's install three R packages. 

```{r packages, results='hide', warning=FALSE, message=FALSE}
library(igraph)
library(tidyverse)
library(gridExtra)

```

The igraph package includes numerous functions that can be used to generate different networks. For example...

```{r built}
# Built in graphs
eg <- make_empty_graph(40)
fg <- make_full_graph(40)
st <- make_star(40)
tr <- make_tree(40, children = 3, mode = "undirected")
rn <- make_ring(40)
lt <- make_lattice(c(4,4,4))

par(mar=c(0,0,2,0), mfrow = c(2, 3))
plot(eg, vertex.size=10, vertex.label=NA,
     main = "Empty Graph")
plot(fg, vertex.size=10, vertex.label=NA, 
     main = "Full Graph")
plot(st, vertex.size=10, vertex.label=NA,
     main = "Star Graph")
plot(rn, vertex.size=10, vertex.label=NA,
     main = "Ring Graph")
plot(tr, vertex.size=10, vertex.label=NA,
     main = "Tree Graph")
plot(lt, vertex.size=10, vertex.label=NA,
     main = "Lattice Graph")
```

An empty graph contains only nodes (or vertices), but no lines (or edges) connecting them. 

A full graph contains every possible line connecting all nodes with one another. 

A star graph contains a central node connected to a series of "pendant" nodes that each have a degree of 1.

A ring graph contains nodes that all have a degree of 2 and that only connect to their nearest "neighbors," generating a circular graph. 

A tree graph contains no transitive triples, which occurs when nodes A, B, and C are all connected together.

A lattice graph contains nodes that exist in a grid structure. 

## Random graphs

In addition to the specific types of graphs, the igraph package allows one to generate random graphs. The way to do this is to use the erdos.renyi.game function. We will need to indicate how many nodes to include in the graph, as well as the odds of tie formation between nodes. Below I specify 40 nodes and tie formation odds at 1 tie for every 15 potential ties (or 6.67%). 

The code below also runs the procedure twice and plots the network graphs side-by-side (see the mfrow option as part of the par command; 1 row and 2 columns in the display). 

```{r random}
r1 <- erdos.renyi.game(40, 1/15)
r2 <- erdos.renyi.game(40, 1/15)
par(mar=c(0,0,2,0), mfrow = c(1, 2))
plot(r1, vertex.size=10, vertex.label=NA,
     main = "Random Graph 1")
plot(r2, vertex.size=10, vertex.label=NA,
     main = "Random Graph 2")

```

Even though the syntax for generating these two graphs is identical, the results show two different graphs. That's because the Erdos-Renyi game procedure is stochastic. In other words, the outcome depends on probability. We provide the general parameters for the graph, but the ultimate features of the graph maintain a random component. 

We will return to these random graphs later in this tutorial. But before we do, we should discuss two other forms of stochastic network simulations. 

## Small Worlds

The principle of Six Degrees of Separation was first established as part of an experiment conducted by Stanley Milgram in the 1960s. He asked research subjects to send mail correspondence to their friends or acquaintances to reach a target person whom they did not know. The study suggested that people, on average, were only six connections away from any other person in the United States. This was a surprising finding indeed. With so many people in the US, it only took 6 connections (again, on average) to reach every person? 

Later research efforts by Duncan Watts pointed to the reason. The social networks of human beings have a unique set of characteristics. First, we have high local density, which simply means that most of the people whom we know also know each other. We tend to form cliques or clans comprised of people who all are relatively interconnected. Second, human social networks also tend to contain individuals who are connected to other individuals outside of their local "neighborhoods." These super connectors make the big world of social networks seem a bit smaller. Just a few of these connections across the system means that it does not take too many connections to get from one end of the network to the other. 

This theory is formalized and can be tested as part of the Watts-Strogatz game. The tendency toward local connectivity is captured by the "nei" option. Here we'll set that as a constant equal to 2. P is the probability of connections outside of the local neighborhood (or in network parlance, "rewiring"). By varying the value of P, we can see how the world gets smaller. Specifically, we can focus on the diameter of the graph to see how large or small the world is. Larger diameters mean that the distance from one end of the graph to the other is greater.  

```{r watts}
par(mar=c(2,1,3,1), mfrow=c(1,3))

P <- .01
g01 <- watts.strogatz.game(dim=1, size=50, nei=2, p=P)
D <- diameter(g01)
plot(g01, vertex.label=NA, layout=layout_in_circle(g01),
     main=paste("p =",P,", Diameter =",D))

P <- .05
g05 <- watts.strogatz.game(dim=1, size=50, nei=2, p=P)
D <- diameter(g05)
plot(g05, vertex.label=NA, layout=layout_in_circle(g05),
     main=paste("p =",P,", Diameter =",D))

P <- .10
g10 <- watts.strogatz.game(dim=1, size=50, nei=2, p=P)
D <- diameter(g10)
plot(g10, vertex.label=NA, layout=layout_in_circle(g10),
     main=paste("p =",P,", Diameter =",D))
```

As P increases, the diameter shrinks. This shows us that the "super connector" feature of human social networks helps to make the world smaller. 

## Preferential attachment

Another very important theory about human social networks is preferential attachment. Consider walking into a room filled with people you have never met before. Who are you most interested in talking with? While some might choose the wallflower over in the corner, most people prefer to meet with the most popular people at the party. 

This principle works the same for social media. The Kardashians don't just have a few more followers than you do on Instagram, they have exponentially more followers than you. (No shame...me, too!) The key insight here is that connections with others follow a "power law" distribution. That is to say, the overwhelming majority of people have a relatively small number of ties, while a small number of people have an immense number of ties. This is all due to preferential attachment. People prefer to attach to others who already have many attachments.

We can illustrate the principle of preferential attachment via the Barabasi game. This function works by generating a random network in stages, adding new nodes and ties to the network in each step. When a new node enters the network, it has a choice of whom to connect with. The choice is stochastic, but new nodes express a preference for connecting with existing nodes that have more connections (i.e., higher degree). 

The code below runs the model. First, I set the number of nodes at 500. Second, I set a value for power at 0.5, which refers to the overall strength of preferences for attachment to popular nodes. I would describe this as a medium to high level of preferencing. You should try different values to see how this alters the results. Finally, I am asking for a non-directed network. 

``` {r barabasi}
b <- barabasi.game(500, power = .5, directed = F)

```

Next, let's visualize the results. We'll set the base color of the nodes as light blue, but then we'll set the popular nodes (degree > 9) to a color of red. Then we can size and scale the nodes by degree. 

``` {r bplot}
V(b)$color <- "lightblue"
V(b)[degree(b)>9]$color <- "red"
node_size <- sqrt(degree(b))+1

par(mar=c(0,0,3,0), mfrow=c(1,1))
plot(b, vertex.label=NA,vertex.size=node_size,
     main=paste("Preference = 0.5"))

```

Note how the structure of this network corresponds to the power law -- many nodes have low degree values and a few nodes have very high degree values. Also, do you see similarities with any of the built-in graphs we generated earlier? How about the tree network? Notice the similar branching structure with few transitive triples. 

## Random walk

One of the things simulations are useful for is respondent-driven sampling. Let's say you meet someone at a party and they tell you that they are part of an exclusive group. This is interesting to you, as a social scientist, which leads to you to interview this person. The interview goes well, so you ask your informant to share with you the name and contact information of another group member. The hope is that you can sequentially interview a few members of the group to understand how the group operates, rather than having to interview all of the members. 

As you may have guessed, a concern here is that you may not be gathering a representative sample of the group members. Who are you likely to be interviewing and who is likely to be left out? Knowing more about the structure of relationships among the group could help you answer this question. 

Let's generate a combined ring and star network. This group contains a popular leader along with a set of weakly-connected peripheral followers. 

``` {r ringostar}
g <- make_ring(10, directed = TRUE) %u%
  make_star(11, center = 11) + edge(11, 1)

plot(g)

```

Given this structure, we can estimate a "random walk" across the network. We can start with our introduction to a peripheral individual (person 1) in the group. Then we can simulate options for who person 1 will nominate for an additional interview. Based on the graph, there are only two options. Person 1 has arrows leading to person 2 and person 11 only. Consequently, the random_walk algorithm randomly chooses either 2 or 11. Then based on that choice, the new focal actor has to make an additional choice. The result is a vector of random walk options from 1 to 10000. 

Below, w presents the randomly sequence of actors selected for an interview. The table shows the times in which each node was selected as a target of a random walk. 

``` {r walk}
w <- random_walk(g, start = 1, steps = 10000)
w
table(w)

```

We can further explore who is most (or least) likely to be nominated for an interview. One might astutely hypothesize, for example, that a group member who is highly central to the network is likely to be nominated for an interview. Below I test this idea using eigenvector centrality. 

``` {r eigen}
ec <- eigen_centrality(g, directed = TRUE)$vector
cor(table(w), ec)

```

The correlation table shows an extraordinarily strong association between a node's eigenvector centrality and their chances of being nominated as part of a random walk. Not surprisingly, central actors are very likely to be nominated for interviews, whereas peripheral actors are less likely to be interviewed. 

## Network distributions

Let's return to the random graph models so we can consider how simulated networks can generate distributions. Previously we used the Erdos-Renyi model to generate two different random networks. Now let's generate 5,000 random networks!

Hold on...why would we want to do that? Well, generating lots of networks creates a set of distributions that can be useful for comparing observed networks to theoretical networks. 

Okay, that explanation probably makes little sense at this point. Let's go ahead and generate these distributions first, then we can consider their usefulness later. 

The code below estimates 5000 random networks using the Erdos-Renyi game function and then saves metrics for four network features: 1) density, 2) transitivity, 3) diameter, and 4) centralization (based on eigenvector centrality scores). The interpretation of these metrics means little right now. The main point is that we're going to get a range of values for these metrics across our large set of networks.

To get into the weeds of the code, gnum is our counter for our looped set of code, which will run 5000 times. gdist is our data frame that starts out empty, but will get filled up as each new graph is estimated. The "for" statement starts the loop, which will run from 1 to 5000. During each iteration, we will estimate a random network and save each of our four metrics. Then we will line the metrics up into a temporary vector (temp) and tack it onto the bottom of the gdist data frame. The resulting data frame will contain 5000 rows and four columns for each of the network metrics. 

``` {r df}
gnum <- c(1:5000)
gdist <- as.data.frame(NULL)

for (i in gnum) {
  g <- erdos.renyi.game(40, 1/15)
  density <- edge_density(g)
  transitivity <- transitivity(g)
  diameter <- diameter(g)
  centralization <- centr_eigen(g)$centralization
  # bind the data and append to the dataframe
  temp <- cbind(density, transitivity, diameter, centralization)
  gdist <- rbind(gdist, temp)
}

```

## Graphing network distributions

Using density plots in ggplot2, we can display the distributional features of the networks. First, I created the 4 graphs, then we can plot them side-by-side using the grid.arrange function. 

``` {r arrange}
p1 <- ggplot(gdist, aes(x = density)) +
  geom_density() +
  geom_vline(xintercept = mean(gdist$density), color="blue")
p2 <- ggplot(gdist, aes(x = transitivity)) +
  geom_density() +
  geom_vline(xintercept = mean(gdist$transitivity), color="blue")
p3 <- ggplot(gdist, aes(x = diameter)) +
  geom_density() +
  geom_vline(xintercept = mean(gdist$diameter), color="blue")
p4 <- ggplot(gdist, aes(x = centralization)) +
  geom_density() +
  geom_vline(xintercept = mean(gdist$centralization), color="blue")

grid.arrange(p1,p2,p3,p4)

```

The graphs show the variation in the features of the randomly generated networks. I added vertical lines to indicate the mean values for each distribution. 

Now we can return to the question of why these distributions are useful. Imagine your professor conducted a survey in your class to identify the extent to which students consider each other to be friends. Now let's say you are examining a network graph of the results. Is this a typical set of relationships? Is it atypical? Are some features of the network typical and others atypical? 

It's difficult to know the answers to these questions without some sort of baseline. That's how random networks can help. By generating random network distributions, we can determine the extent to which an observed network is, say, less centralized than one would expect to find in a random network. Or perhaps transitive triples are more common in an observed network relative to random networks. In these instances, the statistical features of random networks provide a baseline for comparison. 

Consequently, the simulation of random networks provides the basis for inferential statistics in advanced network modeling, such as exponential random graph models (or ERGMs). Interested in learning more about ERGMs? I just so happen to have a separate tutorial on the topic, so please check it out! 



